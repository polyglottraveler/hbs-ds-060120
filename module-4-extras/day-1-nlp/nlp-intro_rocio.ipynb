{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining\n",
    "\n",
    "<img src=\"img/text-miners.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is text mining different? What is text?\n",
    "\n",
    "- Order the words from **SMALLEST** to **LARGEST** units\n",
    " - character\n",
    " - corpora (plural)\n",
    " - sentence\n",
    " - word\n",
    " - corpus (singular)\n",
    " - paragraph\n",
    " - document\n",
    "\n",
    "(after it is all organized)\n",
    "\n",
    "- Any disagreements about the terms used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "## start small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_test = \"Here is a sentence. Or two, I don't think there will be more.\"\n",
    "token_test_2 = \"i thought this sentence was good.\"\n",
    "token_test_3 = \"Here's a sentence... maybe two. Depending on how you like to count!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's tokenize a document... into sentences\n",
    "def make_sentences(doc):\n",
    "\n",
    "make_sentences(token_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'or',\n",
       " 'two,',\n",
       " 'i',\n",
       " 'dont',\n",
       " 'think',\n",
       " 'there',\n",
       " 'will',\n",
       " 'be',\n",
       " 'more']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's tokenize a document into words\n",
    "# with these 3 test cases what would you look out for?\n",
    "def tokenize_it(doc):\n",
    "    new_doc = doc.lower().replace('.', ''). replace(\"'\", '').split(' ')\n",
    "    return new_doc\n",
    "\n",
    "tokenize_it(token_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New library!\n",
    "\n",
    "while we have seen language processing tools in spark, NLTK is its own python library. And of course, it has its own [documentation](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "resp = requests.get('http://www.gutenberg.org/cache/epub/5200/pg5200.txt')\n",
    "metamorph = resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg EBook of Metamorphosis, by Franz Kafka\r\n",
      "Translated by David Wyllie.\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r\n",
      "with this eBook or online at www.gutenberg.net\r\n",
      "\r\n",
      "** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\r\n",
      "**     Please follow the copyright guidelines in this file.     **\r\n",
      "\r\n",
      "\r\n",
      "Title: Metamorphosis\r\n",
      "\r\n",
      "Author: Franz Kafka\r\n",
      "\r\n",
      "Translator: David Wyllie\r\n",
      "\r\n",
      "Release Date: August 16, 2005 [EBook #5200]\r\n",
      "First posted: May 13, 2002\r\n",
      "Last updated: May 20, 2012\r\n",
      "\r\n",
      "Language: English\r\n",
      "\r\n",
      "\r\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Copyright (C) 2002 David Wyllie.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "  Metamorphosis\r\n",
      "  Franz Kafka\r\n",
      "\r\n",
      "Translated by David Wyllie\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "I\r\n",
      "\r\n",
      "\r\n",
      "One morning, when Gregor Samsa woke from troubled dreams, he found\r\n",
      "himself transformed in his bed into a horrible vermin.\n"
     ]
    }
   ],
   "source": [
    "print(metamorph[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your article here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www', 'gutenberg', 'net', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', 'Details', 'Below', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'Title', 'Metamorphosis', 'Author', 'Franz', 'Kafka', 'Translator', 'David', 'Wyllie', 'Release', 'Date', 'August', 'EBook', 'First', 'posted', 'May', 'Last', 'updated', 'May', 'Language', 'English', 'START', 'OF', 'THIS']\n"
     ]
    }
   ],
   "source": [
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "metamorph_tokens_raw = nltk.regexp_tokenize(metamorph, pattern)\n",
    "print(metamorph_tokens_raw[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'metamorphosis', 'by', 'franz', 'kafka', 'translated', 'by', 'david', 'wyllie', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www', 'gutenberg', 'net', 'this', 'is', 'a', 'copyrighted', 'project', 'gutenberg', 'ebook', 'details', 'below', 'please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', 'ebook', 'first', 'posted', 'may', 'last', 'updated', 'may', 'language', 'english', 'start', 'of', 'this']\n"
     ]
    }
   ],
   "source": [
    "metamorph_tokens = [i.lower() for i in metamorph_tokens_raw]\n",
    "print(metamorph_tokens[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rociowu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3029"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(metamorph_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2890"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(metamorph_tokens_stopped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'ebook', 'use', 'anyone', 'anywhere', 'cost', 'almost', 'restrictions', 'whatsoever', 'may', 'copy', 'give', 'away', 'use', 'terms', 'project', 'gutenberg', 'license', 'included', 'ebook', 'online', 'www', 'gutenberg', 'net', 'copyrighted', 'project', 'gutenberg', 'ebook', 'details', 'please', 'follow', 'copyright', 'guidelines', 'file', 'title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', 'ebook', 'first', 'posted', 'may', 'last', 'updated', 'may', 'language', 'english', 'start', 'project', 'gutenberg', 'ebook', 'metamorphosis', 'copyright', 'c', 'david', 'wyllie', 'metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'one', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armour', 'like', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "metamorph_tokens_stopped = [w for w in metamorph_tokens if not w in stop_words]\n",
    "print(metamorph_tokens_stopped[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming / Lemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming - Porter Stemmer \n",
    "<img src=\"https://cdn.homebrewersassociation.org/wp-content/uploads/Baltic_Porter_Feature-600x800.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "example = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'plotted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "singles = [stemmer.stem(e) for e in example]\n",
    "print(*singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming - Snowball Stemmer\n",
    "<img src=\"https://localtvwiti.files.wordpress.com/2018/08/gettyimages-936380496.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic\n",
      "danish\n",
      "dutch\n",
      "english\n",
      "finnish\n",
      "french\n",
      "german\n",
      "hungarian\n",
      "italian\n",
      "norwegian\n",
      "porter\n",
      "portuguese\n",
      "romanian\n",
      "russian\n",
      "spanish\n",
      "swedish\n"
     ]
    }
   ],
   "source": [
    "print(*SnowballStemmer.languages, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "print(stemmer.stem(\"running\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter vs Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous\n",
      "gener\n"
     ]
    }
   ],
   "source": [
    "print(SnowballStemmer(\"english\").stem(\"generously\"))\n",
    "print(SnowballStemmer(\"porter\").stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Snowball on Metamorphosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'metamorphosi', 'franz', 'kafka', 'translat', 'david', 'wylli', 'ebook', 'use', 'anyon', 'anywher', 'cost', 'almost', 'restrict', 'whatsoev', 'may', 'copi', 'give', 'away', 'use', 'term', 'project', 'gutenberg', 'licens', 'includ', 'ebook', 'onlin', 'www', 'gutenberg', 'net', 'copyright', 'project', 'gutenberg', 'ebook', 'detail', 'pleas', 'follow', 'copyright', 'guidelin', 'file', 'titl', 'metamorphosi', 'author', 'franz', 'kafka', 'translat', 'david', 'wylli', 'releas', 'date', 'august', 'ebook', 'first', 'post', 'may', 'last', 'updat', 'may', 'languag', 'english', 'start', 'project', 'gutenberg', 'ebook', 'metamorphosi', 'copyright', 'c', 'david', 'wylli', 'metamorphosi', 'franz', 'kafka', 'translat', 'david', 'wylli', 'one', 'morn', 'gregor', 'samsa', 'woke', 'troubl', 'dream', 'found', 'transform', 'bed', 'horribl', 'vermin', 'lay', 'armour', 'like', 'back', 'lift', 'head', 'littl', 'could', 'see', 'brown', 'belli']\n"
     ]
    }
   ],
   "source": [
    "meta_stemmed = [stemmer.stem(word) for word in metamorph_tokens_stopped]\n",
    "print(meta_stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/rociowu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/rociowu/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 'NN'),\n",
       " ('gutenberg', 'NN'),\n",
       " ('ebook', 'NN'),\n",
       " ('metamorphosis', 'NN'),\n",
       " ('franz', 'NN'),\n",
       " ('kafka', 'NN'),\n",
       " ('translated', 'VBD'),\n",
       " ('david', 'JJ'),\n",
       " ('wyllie', 'NN'),\n",
       " ('ebook', 'NN')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(metamorph_tokens_stopped)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamorph_lemmas_pos = []\n",
    "for x, y in nltk.pos_tag(metamorph_tokens_stopped):\n",
    "    metamorph_lemmas_pos.append((x, get_wordnet_pos(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 'n'),\n",
       " ('gutenberg', 'n'),\n",
       " ('ebook', 'n'),\n",
       " ('metamorphosis', 'n'),\n",
       " ('franz', 'n'),\n",
       " ('kafka', 'n'),\n",
       " ('translated', 'v'),\n",
       " ('david', 'a'),\n",
       " ('wyllie', 'n'),\n",
       " ('ebook', 'n')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metamorph_lemmas_pos[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Lemmatizer on Metamorphosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('slightly', 'slightly')\n",
      "('domed', 'domed')\n",
      "('divided', 'divided')\n",
      "('arches', 'arch')\n",
      "('stiff', 'stiff')\n",
      "('sections', 'section')\n",
      "('bedding', 'bed')\n",
      "('hardly', 'hardly')\n",
      "('able', 'able')\n",
      "('cover', 'cover')\n",
      "('seemed', 'seem')\n",
      "('ready', 'ready')\n",
      "('slide', 'slide')\n",
      "('moment', 'moment')\n",
      "('many', 'many')\n",
      "('legs', 'leg')\n",
      "('pitifully', 'pitifully')\n",
      "('thin', 'thin')\n",
      "('compared', 'compare')\n",
      "('size', 'size')\n",
      "('rest', 'rest')\n",
      "('waved', 'wave')\n",
      "('helplessly', 'helplessly')\n",
      "('looked', 'look')\n",
      "(\"what's\", \"what's\")\n",
      "('happened', 'happen')\n",
      "('thought', 'thought')\n",
      "('dream', 'dream')\n",
      "('room', 'room')\n",
      "('proper', 'proper')\n",
      "('human', 'human')\n",
      "('room', 'room')\n",
      "('although', 'although')\n",
      "('little', 'little')\n",
      "('small', 'small')\n",
      "('lay', 'lay')\n",
      "('peacefully', 'peacefully')\n",
      "('four', 'four')\n",
      "('familiar', 'familiar')\n",
      "('walls', 'wall')\n",
      "('collection', 'collection')\n",
      "('textile', 'textile')\n",
      "('samples', 'sample')\n",
      "('lay', 'lay')\n",
      "('spread', 'spread')\n",
      "('table', 'table')\n",
      "('samsa', 'samsa')\n",
      "('travelling', 'travel')\n",
      "('salesman', 'salesman')\n",
      "('hung', 'hung')\n",
      "('picture', 'picture')\n",
      "('recently', 'recently')\n",
      "('cut', 'cut')\n",
      "('illustrated', 'illustrated')\n",
      "('magazine', 'magazine')\n",
      "('housed', 'house')\n",
      "('nice', 'nice')\n",
      "('gilded', 'gild')\n",
      "('frame', 'frame')\n",
      "('showed', 'show')\n",
      "('lady', 'lady')\n",
      "('fitted', 'fit')\n",
      "('fur', 'fur')\n",
      "('hat', 'hat')\n",
      "('fur', 'fur')\n",
      "('boa', 'boa')\n",
      "('sat', 'sit')\n",
      "('upright', 'upright')\n",
      "('raising', 'raise')\n",
      "('heavy', 'heavy')\n",
      "('fur', 'fur')\n",
      "('muff', 'muff')\n",
      "('covered', 'cover')\n",
      "('whole', 'whole')\n",
      "('lower', 'low')\n",
      "('arm', 'arm')\n",
      "('towards', 'towards')\n",
      "('viewer', 'viewer')\n",
      "('gregor', 'gregor')\n",
      "('turned', 'turn')\n",
      "('look', 'look')\n",
      "('window', 'window')\n",
      "('dull', 'dull')\n",
      "('weather', 'weather')\n",
      "('drops', 'drop')\n",
      "('rain', 'rain')\n",
      "('could', 'could')\n",
      "('heard', 'hear')\n",
      "('hitting', 'hit')\n",
      "('pane', 'pane')\n",
      "('made', 'make')\n",
      "('feel', 'feel')\n",
      "('quite', 'quite')\n",
      "('sad', 'sad')\n",
      "('sleep', 'sleep')\n",
      "('little', 'little')\n",
      "('bit', 'bit')\n",
      "('longer', 'longer')\n",
      "('forget', 'forget')\n",
      "('nonsense', 'nonsense')\n"
     ]
    }
   ],
   "source": [
    "meta_lemmaed = []\n",
    "for word, pos in metamorph_lemmas_pos:\n",
    "    meta_lemmaed.append(lemmatizer.lemmatize(word, pos=pos))\n",
    "print(*zip(metamorph_tokens_stopped[100:200], meta_lemmaed[100:200]), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is a short list of additional considerations when cleaning text:\n",
    "\n",
    "- Handling large documents and large collections of text documents that do not fit into memory.\n",
    "- Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "- Transliteration of characters from other languages into English.\n",
    "- Decoding Unicode characters into a normalized form, such as UTF8.\n",
    "- Handling of domain specific words, phrases, and acronyms.\n",
    "- Handling or removing numbers, such as dates and amounts.\n",
    "- Locating and correcting common typos and misspellings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_freqdist = FreqDist(meta_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gregor', 298),\n",
       " ('would', 187),\n",
       " ('room', 133),\n",
       " ('could', 120),\n",
       " ('work', 114),\n",
       " ('even', 104),\n",
       " ('father', 102),\n",
       " ('sister', 101),\n",
       " ('door', 97),\n",
       " ('gutenberg', 94),\n",
       " ('mother', 90),\n",
       " ('project', 88),\n",
       " ('back', 83),\n",
       " ('one', 76),\n",
       " ('time', 74),\n",
       " ('way', 66),\n",
       " ('look', 61),\n",
       " ('tm', 57),\n",
       " ('open', 56),\n",
       " ('use', 55),\n",
       " ('get', 52),\n",
       " ('said', 51),\n",
       " ('littl', 49),\n",
       " ('go', 49),\n",
       " ('without', 47),\n",
       " ('first', 45),\n",
       " ('still', 45),\n",
       " ('want', 44),\n",
       " ('like', 43),\n",
       " ('see', 42),\n",
       " ('hand', 41),\n",
       " ('made', 40),\n",
       " ('make', 40),\n",
       " ('head', 39),\n",
       " ('much', 39),\n",
       " ('come', 39),\n",
       " ('day', 38),\n",
       " ('thing', 38),\n",
       " ('move', 38),\n",
       " ('chief', 38),\n",
       " ('thought', 37),\n",
       " ('clerk', 37),\n",
       " ('turn', 36),\n",
       " ('away', 35),\n",
       " ('samsa', 34),\n",
       " ('let', 33),\n",
       " ('bed', 32),\n",
       " ('well', 32),\n",
       " ('went', 32),\n",
       " ('famili', 32)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_freqdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fdd8e7be090>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "meta_freqdist.plot(30, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "## this step happens after we account for stopwords and lemmas; depending on the library...\n",
    "* we make a **Count Vector**, which is the formal term for a **bag of words**\n",
    "* we use vectors to pass text into machine learning models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check out the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the CountVectorizer method on 'basic_example'\n",
    "basic_example = ['The Data Scientist wants to train a machine to train machine learning models.']\n",
    "cv = CountVectorizer()\n",
    "cv.fit(basic_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 2, 1, 1, 1, 2, 2, 1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what info can we get from cv?\n",
    "# hint -- look at the docs again\n",
    "\n",
    "cv.transform(basic_example).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'learning',\n",
       " 'machine',\n",
       " 'models',\n",
       " 'scientist',\n",
       " 'the',\n",
       " 'to',\n",
       " 'train',\n",
       " 'wants']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization allows us to compare two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to help see what's happening\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fit the CountVectorizer on the 'basic_example', now we transform 'basic_example'\n",
    "example_vector_doc_1 = cv.transform(basic_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# what is the type\n",
    "\n",
    "print(type(example_vector_doc_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t2\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t2\n",
      "  (0, 7)\t2\n",
      "  (0, 8)\t1\n"
     ]
    }
   ],
   "source": [
    "# what does it look like\n",
    "\n",
    "print(example_vector_doc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>models</th>\n",
       "      <th>scientist</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data  learning  machine  models  scientist  the  to  train  wants\n",
       "0     1         1        2       1          1    1   2      2      1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's visualize it\n",
    "example_vector_df = pd.DataFrame(example_vector_doc_1.toarray(), columns=cv.get_feature_names())\n",
    "example_vector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>models</th>\n",
       "      <th>scientist</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data  learning  machine  models  scientist  the  to  train  wants\n",
       "0     1         0        0       0          1    2   0      0      0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we compare new text to the CountVectorizer fit on 'basic_example'\n",
    "new_text = ['the data scientist plotted the residual error of her model']\n",
    "new_data = cv.transform(new_text)\n",
    "new_count = pd.DataFrame(new_data.toarray(),columns=cv.get_feature_names())\n",
    "new_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this the object 'sentences' becomes the corpus\n",
    "sentences = ['The Data Scientist wants to train a machine to train machine learning models.',\n",
    "             'the data scientist plotted the residual error of her model in her analysis',\n",
    "             'Her analysis was so good, she won a Kaggle competition.',\n",
    "             'The machine gained sentience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back to the docs for count vectorizer, how would we use an ngram\n",
    "# pro tip -- include stop words\n",
    "bigrams = CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x29 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 32 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vector = bigrams.fit_transform(sentences)\n",
    "bigram_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29 features for this corpus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['analysis was',\n",
       " 'data scientist',\n",
       " 'error of',\n",
       " 'gained sentience',\n",
       " 'good she',\n",
       " 'her analysis',\n",
       " 'her model',\n",
       " 'in her',\n",
       " 'kaggle competition',\n",
       " 'learning models']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'There are {str(len(bigrams.get_feature_names()))} features for this corpus')\n",
    "bigrams.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis was</th>\n",
       "      <th>data scientist</th>\n",
       "      <th>error of</th>\n",
       "      <th>gained sentience</th>\n",
       "      <th>good she</th>\n",
       "      <th>her analysis</th>\n",
       "      <th>her model</th>\n",
       "      <th>in her</th>\n",
       "      <th>kaggle competition</th>\n",
       "      <th>learning models</th>\n",
       "      <th>...</th>\n",
       "      <th>she won</th>\n",
       "      <th>so good</th>\n",
       "      <th>the data</th>\n",
       "      <th>the machine</th>\n",
       "      <th>the residual</th>\n",
       "      <th>to train</th>\n",
       "      <th>train machine</th>\n",
       "      <th>wants to</th>\n",
       "      <th>was so</th>\n",
       "      <th>won kaggle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis was  data scientist  error of  gained sentience  good she  \\\n",
       "0             0               1         0                 0         0   \n",
       "1             0               1         1                 0         0   \n",
       "2             1               0         0                 0         1   \n",
       "3             0               0         0                 1         0   \n",
       "\n",
       "   her analysis  her model  in her  kaggle competition  learning models  ...  \\\n",
       "0             0          0       0                   0                1  ...   \n",
       "1             1          1       1                   0                0  ...   \n",
       "2             1          0       0                   1                0  ...   \n",
       "3             0          0       0                   0                0  ...   \n",
       "\n",
       "   she won  so good  the data  the machine  the residual  to train  \\\n",
       "0        0        0         1            0             0         2   \n",
       "1        0        0         1            0             1         0   \n",
       "2        1        1         0            0             0         0   \n",
       "3        0        0         0            1             0         0   \n",
       "\n",
       "   train machine  wants to  was so  won kaggle  \n",
       "0              2         1       0           0  \n",
       "1              0         0       0           0  \n",
       "2              0         0       1           1  \n",
       "3              0         0       0           0  \n",
       "\n",
       "[4 rows x 29 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's visualize it\n",
    "bigram_df = pd.DataFrame(bigram_vector.toarray(), columns=bigrams.get_feature_names())\n",
    "bigram_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "## Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\begin{align}\n",
    "w_{i,j} = tf_{i,j} \\times \\log \\dfrac{N}{df_i} \\\\\n",
    "tf_{i,j} = \\text{number of occurences of } i \\text{ in} j \\\\\n",
    "df_i = \\text{number of documents containing} i \\\\\n",
    "N = \\text{total number of documents}\n",
    "\\end{align} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_sentences = ['The Data Scientist wants to train a machine to train machine learning models.',\n",
    "                    'the data scientist plotted the residual error of her model in her analysis',\n",
    "                    'Her analysis was so good, she won a Kaggle competition.',\n",
    "                    'The machine gained sentiance']\n",
    "# take out stop words\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "# fit transform the sentences\n",
    "tfidf_sentences = tfidf.fit_transform(tf_idf_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize it\n",
    "tfidf_df = pd.DataFrame(tfidf_sentences.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>competition</th>\n",
       "      <th>data</th>\n",
       "      <th>error</th>\n",
       "      <th>gained</th>\n",
       "      <th>good</th>\n",
       "      <th>kaggle</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>model</th>\n",
       "      <th>models</th>\n",
       "      <th>plotted</th>\n",
       "      <th>residual</th>\n",
       "      <th>scientist</th>\n",
       "      <th>sentiance</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "      <th>won</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305288</td>\n",
       "      <td>0.481384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.610575</td>\n",
       "      <td>0.305288</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.325557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325557</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0.325557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.366739</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.486934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  competition      data     error    gained      good    kaggle  \\\n",
       "0  0.000000     0.000000  0.240692  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.325557     0.000000  0.325557  0.412928  0.000000  0.000000  0.000000   \n",
       "2  0.366739     0.465162  0.000000  0.000000  0.000000  0.465162  0.465162   \n",
       "3  0.000000     0.000000  0.000000  0.000000  0.617614  0.000000  0.000000   \n",
       "\n",
       "   learning   machine     model    models   plotted  residual  scientist  \\\n",
       "0  0.305288  0.481384  0.000000  0.305288  0.000000  0.000000   0.240692   \n",
       "1  0.000000  0.000000  0.412928  0.000000  0.412928  0.412928   0.325557   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "3  0.000000  0.486934  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "\n",
       "   sentiance     train     wants       won  \n",
       "0   0.000000  0.610575  0.305288  0.000000  \n",
       "1   0.000000  0.000000  0.000000  0.000000  \n",
       "2   0.000000  0.000000  0.000000  0.465162  \n",
       "3   0.617614  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis was</th>\n",
       "      <th>data scientist</th>\n",
       "      <th>error of</th>\n",
       "      <th>gained sentience</th>\n",
       "      <th>good she</th>\n",
       "      <th>her analysis</th>\n",
       "      <th>her model</th>\n",
       "      <th>in her</th>\n",
       "      <th>kaggle competition</th>\n",
       "      <th>learning models</th>\n",
       "      <th>...</th>\n",
       "      <th>she won</th>\n",
       "      <th>so good</th>\n",
       "      <th>the data</th>\n",
       "      <th>the machine</th>\n",
       "      <th>the residual</th>\n",
       "      <th>to train</th>\n",
       "      <th>train machine</th>\n",
       "      <th>wants to</th>\n",
       "      <th>was so</th>\n",
       "      <th>won kaggle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis was  data scientist  error of  gained sentience  good she  \\\n",
       "0             0               1         0                 0         0   \n",
       "1             0               1         1                 0         0   \n",
       "2             1               0         0                 0         1   \n",
       "3             0               0         0                 1         0   \n",
       "\n",
       "   her analysis  her model  in her  kaggle competition  learning models  ...  \\\n",
       "0             0          0       0                   0                1  ...   \n",
       "1             1          1       1                   0                0  ...   \n",
       "2             1          0       0                   1                0  ...   \n",
       "3             0          0       0                   0                0  ...   \n",
       "\n",
       "   she won  so good  the data  the machine  the residual  to train  \\\n",
       "0        0        0         1            0             0         2   \n",
       "1        0        0         1            0             1         0   \n",
       "2        1        1         0            0             0         0   \n",
       "3        0        0         0            1             0         0   \n",
       "\n",
       "   train machine  wants to  was so  won kaggle  \n",
       "0              2         1       0           0  \n",
       "1              0         0       0           0  \n",
       "2              0         0       1           1  \n",
       "3              0         0       0           0  \n",
       "\n",
       "[4 rows x 29 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compared to bigrams\n",
    "bigram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's test out our TfidfVectorizer\n",
    "test_tdidf = tfidf.transform(['this is a test document','look at me I am a test document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x18 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a vector\n",
    "test_tdidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>competition</th>\n",
       "      <th>data</th>\n",
       "      <th>error</th>\n",
       "      <th>gained</th>\n",
       "      <th>good</th>\n",
       "      <th>kaggle</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>model</th>\n",
       "      <th>models</th>\n",
       "      <th>plotted</th>\n",
       "      <th>residual</th>\n",
       "      <th>scientist</th>\n",
       "      <th>sentiance</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "      <th>won</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  competition  data  error  gained  good  kaggle  learning  \\\n",
       "0       0.0          0.0   0.0    0.0     0.0   0.0     0.0       0.0   \n",
       "1       0.0          0.0   0.0    0.0     0.0   0.0     0.0       0.0   \n",
       "\n",
       "   machine  model  models  plotted  residual  scientist  sentiance  train  \\\n",
       "0      0.0    0.0     0.0      0.0       0.0        0.0        0.0    0.0   \n",
       "1      0.0    0.0     0.0      0.0       0.0        0.0        0.0    0.0   \n",
       "\n",
       "   wants  won  \n",
       "0    0.0  0.0  \n",
       "1    0.0  0.0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tfidf_df = pd.DataFrame(test_tdidf.toarray(), columns=tfidf.get_feature_names())\n",
    "test_tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Similarity Between Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell how similar two documents are to one another, normalizing for size, by taking the cosine similarity of the two. \n",
    "\n",
    "This number will range from [0,1], with 0 being not similar whatsoever, and 1 being the exact same. A potential application of cosine similarity is a basic recommendation engine. If you wanted to recommend articles that are most similar to other articles, you could talk the cosine similarity of all articles and return the highest one.\n",
    "\n",
    "<img src=\"./img/better_cos_similarity.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = CountVectorizer()\n",
    "sunday_afternoon = ['I ate a burger at burger queen and it was very good.',\n",
    "                    'I ate a hot dog at burger prince and it was bad',\n",
    "                    'I drove a racecar through your kitchen door',\n",
    "                    'I ate a hot dog at burger king and it was bad. I ate a burger at burger queen and it was very good']\n",
    "\n",
    "trial.fit(sunday_afternoon)\n",
    "text_data = trial.transform(sunday_afternoon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# the 0th and 2nd index lines are very different, a number close to 0\n",
    "cosine_similarity(text_data[0],text_data[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91413793]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 0th and 3rd index lines are very similar, despite different lengths\n",
    "cosine_similarity(text_data[0],text_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

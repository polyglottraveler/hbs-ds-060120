{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The study of time series has arisen because certain sorts of data streams are heavily dependent on the flow of time. Of course, we have not totally ignored time as a feature up to this point. The selling price of a house probably *does* have some relation to the season or the year as real estate markets grow and decline with certain temporally-indexed economic changes etc. But surely time is not the most important predictor of house price. Square footage would likely be more strongly correlated with price than would date of sale.\n",
    "\n",
    "But there are other sorts of data that more readily lend themselves to a temporal analysis. One canonical example is numbers from a stock exchange: First, data from stock tickers often arrive as numbers anchored to consecutive units of time. I get the selling price for some stock on January 1, say, and the next bit of information I gain will be the selling price for that stock on January 2. (We'll explore this feature of time series below.) Second, and more important, if I'm interested in actually *predicting* the selling price of a stock for, say, tomorrow, then very likely one piece of very salient (i.e. *correlated*) information would be the selling price of that stock *today*.\n",
    "\n",
    "What other examples of this sort of time-dependent data can you think of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data/google-trends_game-of-thrones_us.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will help us load and\n",
    "# clean up a dataset.\n",
    "\n",
    "def load_trend(trend_name='football', country_code='us'):\n",
    "    df = pd.read_csv('data/google-trends_'\n",
    "                     + trend_name + '_'\n",
    "                     + country_code\n",
    "                     + '.csv').iloc[1:, :]\n",
    "    df.columns = ['counts']\n",
    "    df['counts'] = df['counts'].str.replace('<1', '0').astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_trend(**{'trend_name': 'data-science', 'country_code': 'us'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends = [\n",
    "    {'trend_name': 'data-science', 'country_code': 'us'},\n",
    "    {'trend_name': 'football', 'country_code': 'us'},\n",
    "    {'trend_name': 'football', 'country_code': 'uk'},\n",
    "    {'trend_name': 'game-of-thrones', 'country_code': 'us'},\n",
    "    {'trend_name': 'pokemon', 'country_code': 'us'},\n",
    "    {'trend_name': 'taxes', 'country_code': 'us'},   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_dfs = [load_trend(**trend) for trend in trends]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if we can guess which is which just by looking\n",
    "# at their graphs.\n",
    "\n",
    "import matplotlib; matplotlib.style.use('ggplot')\n",
    "\n",
    "fig, axs = plt.subplots(len(trend_dfs), 1, figsize=(8, 10))\n",
    "plt.tight_layout()\n",
    "for i, trend_df in enumerate(trend_dfs):\n",
    "    ax = axs[i]\n",
    "    #ax.set_title(str(trends[i]))\n",
    "    ax.plot(np.array(trend_df.index), trend_df['counts'])\n",
    "    ticks = ax.get_xticks()\n",
    "    ax.set_ylim((0, 100))\n",
    "    ax.set_xticks([tick for tick in ticks if tick%24 == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do a histogram of our data, say of the taxes counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxes_df = load_trend('taxes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(taxes_df['counts']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But clearly we would be missing something important about how the data is structured. Let's try to capture that. We'll stick with the taxes data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a month column\n",
    "\n",
    "taxes_df['i'] = np.arange(len(taxes_df))\n",
    "taxes_df['month'] = taxes_df['i'] % 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using month to predict tax activity\n",
    "\n",
    "trend_model = LinearRegression()\n",
    "trend_model.fit(taxes_df[['i']], taxes_df['counts'])\n",
    "trend_line = trend_model.predict(taxes_df[['i']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_line[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(taxes_df['i'], taxes_df['counts'])\n",
    "plt.plot(taxes_df['i'], trend_line);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this model leaves something to be desired! Let's try again. And this time we'll make explicit use of the time indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_encoder = OneHotEncoder(categories='auto')\n",
    "month_encoder.fit(taxes_df[['month']])\n",
    "month_data = month_encoder.transform(taxes_df[['month']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.hstack((taxes_df[['i']].values, month_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(data, taxes_df['counts'])\n",
    "lr_pred = lr.predict(data)  # Predictive model based on i and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pred[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_df = taxes_df\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.set_title('Taxes')\n",
    "ax.plot(trend_df['i'], trend_df['counts'], label='Data',\n",
    "       linewidth=.5, alpha=.8)\n",
    "ax.plot(trend_df['i'], trend_line, label='Trend')\n",
    "ax.plot(trend_df['i'], lr_pred, label='Regression', linestyle=\"dotted\")\n",
    "plt.legend()\n",
    "ticks = ax.get_xticks()\n",
    "ax.set_ylim((0, 100))\n",
    "ax.set_xticks([tick for tick in ticks if tick%24 == 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = trend_df['counts'] - lr_pred\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.set_title(\"Residuals\")\n",
    "ax.plot(trend_df['i'], trend_df['counts'], label='Data',\n",
    "       linewidth=.5, alpha=.8)\n",
    "ax.plot(trend_df['i'], lr_pred, label='Regression', linestyle=\"dotted\")\n",
    "ax.plot(trend_df['i'], residuals,\n",
    "        label='Residuals', linewidth=.5)\n",
    "\n",
    "#ax.plot(trend_df.index, trend_line, label='trend')\n",
    "plt.legend()\n",
    "ticks = ax.get_xticks()\n",
    "ax.set_ylim((-10, 90))\n",
    "ax.set_xticks([tick for tick in ticks if tick%24 == 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetime Objects\n",
    "\n",
    "These comprise a nice standard way of dealing with times and dates in Python. There is a `datetime` [library](https://docs.python.org/2/library/datetime.html), and inside `pandas` there is a `datetime` module as well as a `to_datetime()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime(2020, 12, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime objects have the parts of a date as attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime(2020, 12, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.timedelta()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moment = datetime.timedelta(minutes=100)\n",
    "moment.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moment.seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.to_datetime('2020-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(['2020-12-30', '2020-12-31'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are [many](https://docs.python.org/2/library/datetime.html) format codes that `datetime` supports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling and Downsampling\n",
    "\n",
    "Sometimes we have information sorted according to a certain level in the temporal hierarchy but we'd like to have it sorted according to a different level. For example, we have monthly data but we want to look at quarters, or we have daily data but we want to visualize hourly trends.\n",
    "\n",
    "**To upsample** is to *increase the frequency* of the data of interest. <br/>\n",
    "**To downsample** is to *decrease the frequency* of the data of interest.\n",
    "\n",
    "There is a `.resample()` method available for `pandas` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.arange(10)\n",
    "np.random.seed(42)\n",
    "target = np.random.random(size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rev_times = pd.to_datetime(times, format='%M')\n",
    "rev_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'target': target}, index=rev_times)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling with `.resample()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resample('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.ffil()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_up = df.resample('S').ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.bfill()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up = df.resample('S').bfill()\n",
    "df_up.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.interpolate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up = df.resample('S').interpolate()\n",
    "df_up.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_up.iloc[60,] - df_up.iloc[0,]) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[df_up.iloc[j+1,].values - df_up.iloc[j,].values for j in range(59)][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling with `.resample()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resample('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_down = df.resample('h').ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://machinelearningmastery.com/resample-interpolate-time-series-data-python/) is a helpful post on this sort of resampling in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposing a Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statsmodels has a great tool for looking at a time series as a sum of parts: a general trend, a seasonality component, and whatever is left over (often called a residual (why?)): its `seasonal_decompose()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxes_df.index = pd.to_datetime(taxes_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(taxes_df['counts'])\n",
    "\n",
    "observed = decomposition.observed\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(411)\n",
    "plt.plot(observed, label='Original', color=\"blue\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend', color=\"blue\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality', color=\"blue\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals', color=\"blue\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise to make sure that the residual really captures *all* remaining information about our times series.\n",
    "\n",
    "For various techincal reasons that won't concern us here, some of the components of the decomposition have NANs at their heads and tails. But we can just use `np.nansum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myst = 0\n",
    "for i in range(len(taxes_df['counts'])):\n",
    "    myst += np.nansum(taxes_df['counts'][i] - trend[i] - seasonal[i] - residual[i])\n",
    "myst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Resources for timeseries manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Aileen Neilsen SciPy - 2016](https://www.youtube.com/watch?v=JNfxr4BQrLk)\n",
    "\n",
    "- [Aileen Neilsen - Github](https://github.com/AileenNielsen/TimeSeriesAnalysisWithPython)\n",
    "\n",
    "- [A project on Flight Fares](https://achyutjoshi.github.io/btp/datacollection)\n",
    "\n",
    "- [Python DS Handbook - Working with TS](https://jakevdp.github.io/PythonDataScienceHandbook/03.11-working-with-time-series.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Agenda__\n",
    "\n",
    "1. Introduction to unsupervised learning\n",
    "\n",
    "2. Clustering\n",
    "\n",
    "3. Kmeans algorithm details\n",
    "\n",
    "4. Implementation of kmeans with sklearn\n",
    "\n",
    "5. How to choose number of clusters: Silhouette & Calinski-Harabasz score\n",
    "\n",
    "6. Challenge\n",
    "\n",
    "7. An interesting application of the kmeans algorithm with image processing.\n",
    "\n",
    "8. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "- Association Rules\n",
    "\n",
    "- Cluster Analysis\n",
    "\n",
    "- Principal Components, Curves and Surfaces\n",
    "\n",
    "- Indepedent Component Analysis\n",
    "\n",
    "- Multidimensional Scaling\n",
    "\n",
    "- Non-linear Dimension Reduction\n",
    "\n",
    "<img src=\"img/map_of_ml.png\" width=650, height=650> \n",
    "\n",
    "[Img source](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    "\n",
    "\n",
    "## Clustering\n",
    "\n",
    "A clustering problem is where you want to discover the inherent groupings in the data.\n",
    "\n",
    "## K-Means  Algorithm\n",
    "\n",
    "\n",
    "<img src=\"img/kmeans.png\" width=650, height=650> \n",
    "\n",
    "\n",
    "[Let's see kmeans in action](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)\n",
    "\n",
    "\n",
    "[This notebook is motivated from](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(110119)\n",
    "\n",
    "X, y = make_blobs(n_samples=700, n_features=2, centers=4, cluster_std=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can you plot this dataset\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's instantiate kmeans algorithm\n",
    "# don't forget to check its parameters\n",
    "k_means = KMeans(n_clusters=4)\n",
    "\n",
    "# dont forget to fit the model!\n",
    "k_means.fit(X)\n",
    "\n",
    "# we make a prediction for each point\n",
    "y_hat = k_means.predict(X)\n",
    "\n",
    "# we can access the coordinates of the cluster centers by cluster_centers_ method\n",
    "cl_centers = k_means.cluster_centers_\n",
    "\n",
    "# note that the colors are different - Is this a problem?\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_hat, s=25)\n",
    "\n",
    "\n",
    "# also let's mark the cluster centers too.\n",
    "plt.scatter(cl_centers[:, 0], cl_centers[:, 1], c='black', s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__\n",
    "\n",
    "- Guess how many cluster are there in the figure below.\n",
    "\n",
    "- Use kmeans to find clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('blobs_1.obj', 'rb')\n",
    "data = pickle.load(dbfile)\n",
    "\n",
    "X = data[0]\n",
    "\n",
    "# can you plot this dataset\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compare your results with the actual values below.__\n",
    "\n",
    "- Do they close to the actual values?\n",
    "\n",
    "- What might go wrong?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's play with cluster_std and try to find number of clusters\n",
    "\n",
    "np.random.seed(110119)\n",
    "\n",
    "X, y = make_blobs(n_samples=700, n_features=2, centers=np.array([[-10, -20],\n",
    "                                                                 [-5, -15],\n",
    "                                                                 [-2, -9],\n",
    "                                                                 [5, 12],\n",
    "                                                                 [7, 17]\n",
    "                                                                 ]), cluster_std=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How do we find optimal K value?\n",
    "\n",
    "[Metrics](https://scikit-learn.org/stable/modules/clustering.html#k-mean)\n",
    "\n",
    "[Calinski_Harabasz](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index)\n",
    "\n",
    "[Silhoutte Coefficients](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "\n",
    "visualizer = KElbowVisualizer(\n",
    "    model, k=(2, 10), metric='calinski_harabasz', timings=False)\n",
    "\n",
    "visualizer.fit(X)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(\n",
    "    model, k=(2, 10), metric='calinski_harabasz', timings=False\n",
    ")\n",
    "\n",
    "visualizer.fit(X)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model,\n",
    "                              k=(2, 10),\n",
    "                              metric='silhouette',\n",
    "                              timings=False,\n",
    "                              locate_elbow=True)\n",
    "\n",
    "visualizer.fit(X)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Yellowbrick API](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Clustering in action\n",
    "\n",
    "**[This post here](https://www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/)** walks through cluster assignment _step_ by _step_ if the demo would be helpful.\n",
    "\n",
    "Meanwhile, we can do it in _**scipy**_ and _**sklearn**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hierarchical clustering with `scipy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# lets generate some data and look at an example of hierarchical agglomerative clustering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# generate two clusters: a with 100 points, b with 50:\n",
    "np.random.seed(1000)\n",
    "a = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[50, ])\n",
    "b = np.random.multivariate_normal([0, 20], [[3, 1], [1, 4]], size=[50, ])\n",
    "X = np.concatenate((a, b),)\n",
    "print(X.shape)  # 150 samples with 2 dimensions\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.title(\"Sample data for clustering demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# construct dendrogram in scipy\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "Z = linkage(X, 'single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "c, coph_dists = cophenet(Z, pdist(X))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# calculate and construct the dendrogram\n",
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.,  # font size for the x axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# trimming and truncating the dendrogram\n",
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=12,  # show only the last p merged clusters\n",
    "    show_leaf_counts=False,  # otherwise numbers in brackets are counts\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# from documentation of \"lastp\"\n",
    "# The last p non-singleton formed in the linkage are the only non-leaf nodes in the linkage;\n",
    "# they correspond to rows Z[n-p-2:end] in Z. All other non-singleton clusters are contracted into leaf nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hierarchical clustering with `sklearn` on Iris (because it's there)\n",
    "\n",
    "**[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)** for AgglomerativeClustering in `sklearn`\n",
    "\n",
    "\n",
    "**[A great example of using manhattan distance](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering_metrics.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-metrics-py)** with agglomerative clustering in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use the scikitlearn module hierarchical clustering to perform the same task\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import KernelDensity\n",
    "np.random.seed(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# try clustering on the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "# in this case, we won't be working with predicting labels, so we will only use the features (X)\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_iris[:, 0], X_iris[:, 2])  # c = y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "iris_cluster = AgglomerativeClustering(n_clusters=3)\n",
    "iris_cluster\n",
    "pred_iris_clust = iris_cluster.fit_predict(X_iris)\n",
    "plt.scatter(X_iris[:, 0], X_iris[:, 2], c=pred_iris_clust, s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# compare it to the actual truth\n",
    "plt.scatter(X_iris[:, 0], X_iris[:, 2], c=y_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Evaluate\n",
    "\n",
    "To evaluate you might try different numbers of clusters and compare their silhouette score as you did w kmeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation - silhouette score\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(X_iris, pred_iris_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating number of clusters / Cut points\n",
    "For hierarchical agglomerative clustering, or clustering in general, it is generally difficult to truly evaluate the results. Therefore, it is up you, the data scientists, to decide.\n",
    "\n",
    "**[Standford has a good explaination on page 380](https://nlp.stanford.edu/IR-book/pdf/17hier.pdf)** of your options for picking the cut-off. \n",
    "\n",
    "When we are viewing dendrograms for hierarchical agglomerative, we can visually examine where the natural cutoff is, despite it not sounding exactly statistical, or scientific. We might want to interpret the clusters and assign meanings to them depending on domain-specific knowledge and shape of dendrogram. However, we can evaluate the quality of our clusters using measurements such as Sihouette score discussed in the k-means lectures. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages & Disadvantages of hierarchical clustering\n",
    "\n",
    "#### Advantages\n",
    "- Intuitive and easy to implement\n",
    "- More informative than k-means because it takes individual relationship into consideration\n",
    "- Allows us to look at dendrogram and decide number of clusters\n",
    "\n",
    "#### Disadvantages\n",
    "- Very sensitive to outliers\n",
    "- Cannot undo the previous merge, which might lead to problems later on \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further reading\n",
    "\n",
    "- [from MIT on just hierarchical](http://web.mit.edu/6.S097/www/resources/Hierarchical.pdf)\n",
    "- [from MIT comparing clustering methods](http://www.mit.edu/~9.54/fall14/slides/Class13.pdf)\n",
    "- [fun CMU slides on clustering](http://www.cs.cmu.edu/afs/andrew/course/15/381-f08/www/lectures/clustering.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Find those clusters!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop = pd.read_excel('Online Retail.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

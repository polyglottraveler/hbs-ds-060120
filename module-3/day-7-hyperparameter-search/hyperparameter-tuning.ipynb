{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "- Assess the performance of machine learning models\n",
    "\n",
    "- Diagnose the common problems in machine learning algorithms \n",
    "\n",
    "- Evaluate the predictive models using the different performance metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data',\n",
    "                 header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the independent variables as X and the column 1 as dependent variable. Use LabelEncoder for converting labels into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:,2:].values\n",
    "y = df.loc[:, 1].values\n",
    "le = LabelEncoder() # instantiate LabelEncoder \n",
    "y = le.fit_transform(y) # Fit le object and then transform labels to integers \n",
    "print(' Actual y labels: {}\\n'.format(df.loc[:, 1].values[:5]),\n",
    "      'Transformed y values: {}\\n'.format(y[:5]),\n",
    "      'All labels available in the data {}'.format(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.unique(y, return_counts = True)) # number of 1's and 0's after transformation\n",
    "print(np.unique(df.loc[:, 1], return_counts = True)) # Counts for each label in the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train test split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pipelines: Transformers and Estimators__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing tools\n",
    "from sklearn.preprocessing import StandardScaler # for scaling the features\n",
    "from sklearn.preprocessing import PolynomialFeatures # for checking interaction effect between features\n",
    "\n",
    "## models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "    \n",
    "# for pipelines\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's start with Logistic Regression'\n",
    "\n",
    "log_pipe = make_pipeline(StandardScaler(), # if we want to use regularization we need scaler \n",
    "                         PolynomialFeatures(degree=2, interaction_only= True), # we will only check the interactions, \n",
    "                         LogisticRegression(random_state = 1, solver = 'lbfgs'))\n",
    "\n",
    "## Without further ado check the baseline\n",
    "\n",
    "log_pipe.fit(X_train, y_train) # pipe behaves like sklearn estimator.\n",
    "\n",
    "y_pred = log_pipe.predict(X_train) # predictions of vanilla log_reg model.\n",
    "\n",
    "log_pipe.score(X_train, y_train) # score looks pretty impressive can we expect similar performance on the test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__An overview look for the pipelines__\n",
    "\n",
    "<img src='img/pipelines.png' width = 450/>\n",
    "\n",
    "[Source: Python Machine Learning](https://www.amazon.com/dp/1789955750?tag=duckduckgo-ffab-20&linkCode=osi&th=1&psc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cross validation with pipelines__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Holdout Cross_Validation__\n",
    "<img src= 'img/cross_validation.png' width = 450/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kfold Cross_Validation__\n",
    "\n",
    "<img src = 'img/kfold_cross.png' width= 450/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Understanding Over or Underfitting__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "parameters = [0.01, 0.1, 1, 10, 100] # we will be checking the regularization parameter in Log_reg\n",
    "\n",
    "# we could do the same thing with\n",
    "\n",
    "parameters = np.logspace(-3,2,5)\n",
    "\n",
    "np.set_printoptions(suppress= True)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the train and validation scores are changing as we change C - Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores, test_scores = validation_curve(estimator=log_pipe,\n",
    "                                             X=X_train,\n",
    "                                             y=y_train,\n",
    "                                             # this is the way for accessing a parameter of a\n",
    "                                             param_name='logisticregression__C',\n",
    "                                             # transformer within pipeline\n",
    "                                             param_range=parameters,\n",
    "                                             cv=10,  # note that this can take too long if your data is big\n",
    "                                             verbose=1,  # algorithms will update us about the progress\n",
    "                                             n_jobs=-1  # we will be using the other processing units in parallel\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simply plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(parameters, train_scores.mean(axis= 1), label = 'train')\n",
    "plt.xscale('log')\n",
    "plt.plot(parameters, test_scores.mean(axis = 1), label = 'test')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fine-Tuning ML models via gridsearch__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch approach is very straight forward.\n",
    "\n",
    "__Step1:__ Decide an estimator to use.\n",
    "\n",
    "Suppose we would like to use a decision_tree classifier.\n",
    "\n",
    "__Step2:__ Create a parameter grid\n",
    "\n",
    "Suppose for the decision trees we would like to find best values for: \n",
    "\n",
    "max_depth = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "and \n",
    "\n",
    "max_features = ['auto', 'sqrt', 'log2', 25]\n",
    "\n",
    "param_grid = {'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "              'max_features': ['auto', 'sqrt', 'log2', 25]\n",
    "              }\n",
    "              \n",
    "__Step3:__ Instantiate GridSearchCV with these parameters.\n",
    "\n",
    "__Step4:__ Fit gridsearchcv object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(min_samples_leaf=10)\n",
    "\n",
    "max_depth_params = range(2, 12)  # values between 2 to 11 total: 10 values\n",
    "\n",
    "max_features_param = [None, 'auto', 'sqrt', 'log2', 25]  # total of 5 values\n",
    "\n",
    "param_grid = {'max_depth': max_depth_params,\n",
    "              'max_features': max_features_param}\n",
    "\n",
    "gridsearch = GridSearchCV(estimator=tree_clf,\n",
    "                          param_grid=param_grid,\n",
    "                          n_jobs=-1,  # paralllel computation\n",
    "                          verbose=1,  # gives feedback\n",
    "                          cv=10,  # cross-validate\n",
    "                          scoring='roc_auc',  # you can use multiple scoring too\n",
    "                          return_train_score=True)\n",
    "\n",
    "gridsearch = gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gridsearch.best_score_) # note if you use multiple this doesn't work\n",
    "print(gridsearch.best_estimator_) # doesn't work for multiple scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_df = pd.DataFrame(gridsearch.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colums = ['params', 'mean_test_score', \n",
    "          'std_test_score', 'rank_test_score',\n",
    "          'mean_train_score', 'std_train_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that gridsearch.best_estimator is an decisiontreeclassifier object\n",
    "# so score returns 'accuracy' by default\n",
    "gridsearch.best_estimator_.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_df[colums].sort_values(by = 'rank_test_score').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_test_scores = gridsearch.cv_results_['mean_test_score']\n",
    "\n",
    "roc_training_scores = gridsearch.cv_results_['mean_train_score']\n",
    "\n",
    "plt.plot(range(50), roc_test_scores, label = 'test')\n",
    "plt.plot(range(50), roc_training_scores, label = 'train')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use pipelines with GridSearchCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'logisticregression__C': np.logspace(-3, 2, 10),  \n",
    "              'polynomialfeatures__interaction_only': [True, False]}\n",
    "\n",
    "\n",
    "gridsearch = GridSearchCV(estimator = log_pipe, \n",
    "                          param_grid = param_grid,\n",
    "                          n_jobs = -1, \n",
    "                          verbose = 1,\n",
    "                          cv = 10, \n",
    "                          scoring = 'roc_auc', \n",
    "                          return_train_score= True)\n",
    "\n",
    "gridsearch = gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's see best score and best parameters\n",
    "\n",
    "gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colums = ['params', 'mean_test_score', \n",
    "          'std_test_score', 'rank_test_score',\n",
    "          'mean_train_score', 'std_train_score']\n",
    "\n",
    "log_reg_results = pd.DataFrame(gridsearch.cv_results_)[colums]\n",
    "log_reg_results.sort_values(by = 'rank_test_score').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_test_scores = gridsearch.cv_results_['mean_test_score']\n",
    "\n",
    "roc_training_scores = gridsearch.cv_results_['mean_train_score']\n",
    "\n",
    "plt.plot(range(20), roc_test_scores, label = 'test')\n",
    "plt.plot(range(20), roc_training_scores, label = 'train')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator= log_pipe, param_grid = param_grid, scoring = 'roc_auc', cv = 2 )\n",
    "\n",
    "scores = cross_val_score(gs, X_train, y_train, scoring = 'roc_auc', cv = 5)\n",
    "\n",
    "print('CV accuracy: %.3f +/- %.3f'%(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = gridsearch.best_estimator_.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Faster Hyperparameter tuning: Randomized Approach](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "\n",
    "[Learning the hyperparameter space](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a)\n",
    "\n",
    "[Using sklearn for plotting learning curves](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html)\n",
    "\n",
    "[YellowBrick Validation Curve](https://www.scikit-yb.org/en/latest/api/model_selection/validation_curve.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Gradient Boosting\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Preliminaries:\n",
    "    * Regression & Classification\n",
    "    * Decision Trees & Forests\n",
    "    * Logistic Regression\n",
    "    * Differential Calculus\n",
    "\n",
    "\n",
    "* For Scorecard Extension:\n",
    "    * Logistic Regression\n",
    "    * One-Hot Encoding\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this lecture a student should be able to:\n",
    "\n",
    "* Explain why gradient boosted trees are more prone to overfitting than forests.\n",
    "* Explain why gradient boosting is still relatively robust to overfitting and what some of the nuances may be.\n",
    "* Explain why we use early stopping criteria and reduced learning rates.\n",
    "* Create a gradient boosted classifier using:\n",
    "    * Scikit-Learn\n",
    "    * XGBoost\n",
    "    * LightGBM\n",
    "    * etc.\n",
    "\n",
    "Above and Beyond:\n",
    "\n",
    "* Explain how to create a decision tree proxy (surrogate model) of a \"black-box\" model and its potential benefits and downsides.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import KBinsDiscretizer, FunctionTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "\n",
    "# for reference\n",
    "class TemplateClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, demo_param='demo'):\n",
    "        self.demo_param = demo_param\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "\n",
    "        closest = np.argmin(euclidean_distances(X, self.X_), axis=1)\n",
    "        return self.y_[closest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class ForestEncoder(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, forest):\n",
    "        self.forest = forest\n",
    "        self.n_trees = 1\n",
    "        try:\n",
    "            self.n_trees = self.forest.n_estimators\n",
    "        except:\n",
    "            pass\n",
    "        self.ohe = OneHotEncoder(cols=range(self.n_trees), use_cat_names=True)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.forest.fit(X, y)\n",
    "        self.ohe.fit(self.forest.apply(X))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return self.ohe.transform(self.forest.apply(X))\n",
    "\n",
    "    \n",
    "# TO-DO\n",
    "class SurrogateClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, base_model, proxy_model, base_size, test_size, random_state=0):\n",
    "        self.base_model = base_model\n",
    "        self.proxy_model = proxy_model\n",
    "        self.base_size = base_size\n",
    "        self.test_size = test_size\n",
    "        self.proxy_size = 1 - base_size - test_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Too Many Dimensions to Visualize (X: 10-dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, \n",
    "                           n_features=10, n_informative=5, n_redundant=2, \n",
    "                           n_classes=2, n_clusters_per_class=3, \n",
    "                           weights=[0.3], random_state=42)\n",
    "\n",
    "\n",
    "cols = [\"X\"+str(i) for i in range(10)]\n",
    "df_10 = pd.DataFrame(X, columns=cols)\n",
    "target = 'target'\n",
    "df_10[target] = y\n",
    "\n",
    "samp = df_10.sample(100)\n",
    "\n",
    "sns.pairplot(samp, vars=cols, \n",
    "             hue=target, diag_kind='kde', \n",
    "             plot_kws={'alpha': 0.6, 's': 40},\n",
    "             height=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Forest-Scorecard Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = [c for c in df_10.columns.tolist() if c not in [target]]\n",
    "X, y = df_10[used_cols], df_10[target]\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
    "                            criterion='entropy', max_depth=10, max_features=5,\n",
    "                            min_samples_leaf=3, min_samples_split=5,\n",
    "                            n_estimators=71, n_jobs=-1, random_state=42)\n",
    "\n",
    "encoder = ForestEncoder(rf)\n",
    "clf = LogisticRegression(class_weight='balanced')\n",
    "pipe = make_pipeline(encoder, clf)\n",
    "\n",
    "scores = cross_val_score(pipe, X, y, cv=5, scoring='roc_auc')\n",
    "print(scores.mean(), \"+/-\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "## Gradient Boosting with SciKit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_hist_gradient_boosting \n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Model Baselines\n",
    "\n",
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = [c for c in df_10.columns.tolist() if c not in [target]]\n",
    "X, y = df_10[used_cols], df_10[target]\n",
    "\n",
    "svm = SVC(probability=True, class_weight='balanced', random_state=42)\n",
    "\n",
    "scores = cross_val_score(svm, X, y, cv=5, scoring='roc_auc')\n",
    "print(scores.mean(), \"+/-\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = [c for c in df_10.columns.tolist() if c not in [target]]\n",
    "X, y = df_10[used_cols], df_10[target]\n",
    "\n",
    "scores = cross_val_score(rf, X, y, cv=5, scoring='roc_auc')\n",
    "print(scores.mean(), \"+/-\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted Models\n",
    "\n",
    "#### AdaBoosted SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(svm, n_estimators=50, \n",
    "                         learning_rate=0.1, random_state=42)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "print(scores.mean(), \"+/-\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=2000,\n",
    "                                 learning_rate=0.1, random_state=42, \n",
    "                                 subsample=0.9, max_depth=5, max_features=5,\n",
    "                                 min_samples_leaf=2, min_samples_split=5,\n",
    "                                 validation_fraction=0.20, # use 20% of the data as hold-out for early stopping\n",
    "                                 n_iter_no_change=50, # allowed to go 50 iterations without improvement to hold-out score\n",
    "                                 verbose=0)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "print(scores.mean(), \"+/-\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.fit(X, y)\n",
    "# clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We will need to tune this to beat the performance of the tuned forest.\n",
    "\n",
    "#### Histogram-Gradient Boosted Trees (akin to the LightGBM algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = HistGradientBoostingClassifier(loss='binary_crossentropy', random_state=42, \n",
    "                                     learning_rate=0.05, max_iter=150, \n",
    "                                     max_leaf_nodes=2**5-1, max_bins=100, l2_regularization=0.5)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "print(scores.mean(), \"+/-\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngboost import NGBClassifier\n",
    "from ngboost.distns import Bernoulli\n",
    "from ngboost.learners import default_tree_learner\n",
    "from ngboost.scores import MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngb = NGBClassifier(Base=default_tree_learner, \n",
    "                    Dist=Bernoulli, \n",
    "                    Score=MLE, verbose=False)\n",
    "\n",
    "scores = cross_val_score(ngb, X, y, cv=5, scoring='roc_auc')\n",
    "print(scores.mean(), \"+/-\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We need to tune the default hyper-parameters to improve upon this score.\n",
    "\n",
    "# Surrogate Modeling\n",
    "\n",
    "We can take a \"black-box\" model such as a GBM and make it more interpretable by approximating with a simpler model like a DT.\n",
    "\n",
    "## Original Model and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = [c for c in df_10.columns.tolist() if c not in [target]]\n",
    "X, y = df_10[used_cols], df_10[target]\n",
    "\n",
    "# original model\n",
    "clf = AdaBoostClassifier(svm, n_estimators=50, \n",
    "                         learning_rate=0.1, \n",
    "                         random_state=42)\n",
    "\n",
    "# 45 - 45 - 10 Split : Train - Proxy - Hold-out\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)\n",
    "\n",
    "X_train, X_proxy, y_train, y_proxy = train_test_split(X_train, y_train, test_size=0.50, \n",
    "                                                      random_state=42, \n",
    "                                                      stratify=y_train)\n",
    "\n",
    "# fit the original model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# score on hold-out set\n",
    "y_pred = clf.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probabilities on the \"proxy\" hold-out set\n",
    "y_pred = clf.predict_proba(X_proxy)[:,1]\n",
    "\n",
    "# surrogate / proxy - DT approximation of Original model\n",
    "dt = DecisionTreeRegressor(max_leaf_nodes=2**4-1, random_state=42)\n",
    "dt.fit(X_proxy, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt.predict(X_test)\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Surrogate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "plot_tree(dt, feature_names=used_cols, filled=True,\n",
    "          leaves_parallel=True, node_ids=True, rotate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References / Resources\n",
    "\n",
    "* SciKit-Learn Implementations\n",
    "\n",
    "    * https://scikit-learn.org/stable/modules/ensemble.html#adaboost\n",
    "\n",
    "    * https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting\n",
    "\n",
    "    * https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting\n",
    "\n",
    "* Non-SciKit-Learn Implementations\n",
    "\n",
    "    * https://github.com/catboost/catboost\n",
    "\n",
    "    * https://github.com/microsoft/LightGBM\n",
    "\n",
    "    * https://github.com/jhwjhw0123/Imbalance-XGBoost\n",
    "\n",
    "    * https://github.com/dmlc/xgboost\n",
    "\n",
    "    * https://github.com/stanfordmlgroup/ngboost\n",
    "\n",
    "* Background\n",
    "\n",
    "    * https://en.wikipedia.org/wiki/Gradient_boosting\n",
    "\n",
    "    * https://explained.ai/gradient-boosting/index.html\n",
    "\n",
    "    * https://en.wikipedia.org/wiki/AdaBoost\n",
    "\n",
    "    * https://en.wikipedia.org/wiki/XGBoost\n",
    "\n",
    "* Other\n",
    "\n",
    "    * https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html#sphx-glr-auto-examples-ensemble-plot-feature-transformation-py\n",
    "\n",
    "    * https://github.com/szilard/GBM-perf\n",
    "\n",
    "    * https://github.com/talperetz/awesome-gradient-boosting#notebooks\n",
    "\n",
    "    * https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/dt_surrogate_loco.ipynb\n",
    "    \n",
    "    * https://github.com/limexp/xgbfir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
